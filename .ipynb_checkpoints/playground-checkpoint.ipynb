{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName(\"sparkify\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read song data file\n",
    "df = spark.read.json('data/song_data/*/*/*/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|  artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARDR4AC1187FB371A1|           null|                 |            null|Montserrat Caball...|511.16363|        1|SOBAYLL12A8C138AF9|Sono andati? Fing...|   0|\n",
      "|AREBBGV1187FB523D2|           null|      Houston, TX|            null|Mike Jones (Featu...|173.66159|        1|SOOLYAZ12A6701F4A6|Laws Patrolling (...|   0|\n",
      "|ARMAC4T1187FB3FA4C|       40.82624|Morris Plains, NJ|       -74.47995|The Dillinger Esc...|207.77751|        1|SOBBUGU12A8C13E95D|Setting Fire to S...|2004|\n",
      "|ARPBNLO1187FB3D52F|       40.71455|     New York, NY|       -74.00712|            Tiny Tim| 43.36281|        1|SOAOIBZ12AB01815BE|I Hold Your Hand ...|2000|\n",
      "|ARDNS031187B9924F0|       32.67828|          Georgia|       -83.22295|          Tim Wilson|186.48771|        1|SONYPOM12A8C13B2D7|I Think My Wife I...|2005|\n",
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"data-lake-project/data/partitioned/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Song Data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table = df.select('song_id', \n",
    "                        'title', \n",
    "                        'artist_id', \n",
    "                        'year', \n",
    "                        'duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fing...|ARDR4AC1187FB371A1|   0|511.16363|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table.write.parquet(os.path.join(output_path, '/artists'), mode='overwrite', partitionBy=('year', 'artist_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView('songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table = df.selectExpr('artist_id as id', \n",
    "                            'artist_name as name', \n",
    "                            'artist_location as location', \n",
    "                            'artist_latitude as latitude', \n",
    "                            'artist_longitude as longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_table.write.parquet(os.path.join(output_path, '/artists'), mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Song Data Views of the files in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|  artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARDR4AC1187FB371A1|           null|                 |            null|Montserrat Caball...|511.16363|        1|SOBAYLL12A8C138AF9|Sono andati? Fing...|   0|\n",
      "|AREBBGV1187FB523D2|           null|      Houston, TX|            null|Mike Jones (Featu...|173.66159|        1|SOOLYAZ12A6701F4A6|Laws Patrolling (...|   0|\n",
      "|ARMAC4T1187FB3FA4C|       40.82624|Morris Plains, NJ|       -74.47995|The Dillinger Esc...|207.77751|        1|SOBBUGU12A8C13E95D|Setting Fire to S...|2004|\n",
      "+------------------+---------------+-----------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM songs_table\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Log data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "|     artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|           song|status|           ts|           userAgent|userId|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "|   Harmonia|Logged In|     Ryan|     M|            0|   Smith|655.77751| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|  Sehr kosmisch|   200|1542241826796|\"Mozilla/5.0 (X11...|    26|\n",
      "|The Prodigy|Logged In|     Ryan|     M|            1|   Smith|260.07465| free|San Jose-Sunnyval...|   PUT|NextSong|1.541016707796E12|      583|The Big Gundown|   200|1542242481796|\"Mozilla/5.0 (X11...|    26|\n",
      "+-----------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+---------------+------+-------------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_data_frame = spark.read.json('data/log-data/*.json')\n",
    "log_data_frame.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data_frame.createTempView('log_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log_data_frame = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM log_data\n",
    "    WHERE page = 'NextSong'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table = new_log_data_frame.selectExpr('userId as user_id', \n",
    "                                            'firstName as first_name', \n",
    "                                            'lastName as last_name', \n",
    "                                            'gender', \n",
    "                                            'level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table.write.parquet(os.path.join(output_path, '/users'), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datetime column from original timestamp column\n",
    "get_timestamp = udf(lambda ts: datetime.datetime.fromtimestamp(ts/1000.0), TimestampType())\n",
    "new_table_datetime_converted = new_log_data_frame.withColumn('ts', get_timestamp(new_log_data_frame.ts)).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(ts)>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('get_timestamp', get_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table = new_table_datetime_converted.selectExpr('ts as start_time', \n",
    "                                                     'extract(hour from ts) as hour',\n",
    "                                                     'extract(day from ts) as day', \n",
    "                                                     'weekofyear(ts) as week', \n",
    "                                                     'extract(month from ts) as month', \n",
    "                                                     'extract(year from ts) as year', \n",
    "                                                     'weekday(ts) as weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_table.write.parquet(os.path.join(output_path, 'times'), partitionBy=('year', 'month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "songplay_table = spark.sql(\"\"\"\n",
    "    SELECT ts as start_time, userId as user_id, level, song, sessionId as session_id, location, userAgent as user_agent, \n",
    "    year(get_timestamp(ts)) as year, month(get_timestamp(ts)) as month\n",
    "    FROM log_data\n",
    "    JOIN songs_table\n",
    "    ON log_data.artist == songs_table.artist_name\n",
    "    AND log_data.song == songs_table.title\n",
    "    AND log_data.length == songs_table.duration\n",
    "\"\"\").withColumn('songplay_id', monotonically_increasing_id())\n",
    "# .withColumn('year', year('start_time')).withColumn('month', month('start_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+--------------+----------+--------------------+--------------------+----+-----+-----------+\n",
      "|   start_time|user_id|level|          song|session_id|            location|          user_agent|year|month|songplay_id|\n",
      "+-------------+-------+-----+--------------+----------+--------------------+--------------------+----+-----+-----------+\n",
      "|1542837407796|     15| paid|Setanta matins|       818|Chicago-Napervill...|\"Mozilla/5.0 (X11...|2018|   11|          0|\n",
      "+-------------+-------+-----+--------------+----------+--------------------+--------------------+----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplay_table.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplay_table.write.parquet(os.path.join(output_path, 'songplay'), mode='overwrite', partitionBy=('year', 'month'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
